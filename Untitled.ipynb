{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73bbf366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import env\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f87d16d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_zillow():\n",
    "    \"\"\" Acquires the Zillow housing data from the SQL database or a cached CSV file. Renames columns and outputs data as a Pandas DataFrame\"\"\"\n",
    "    # Acquire data from CSV if exists\n",
    "    if os.path.exists('zillow_2017.csv'):\n",
    "        print(\"Using cached data\")\n",
    "        df = pd.read_csv('zillow_2017.csv')\n",
    "    # Acquire data from database if CSV does not exist\n",
    "    else:\n",
    "        print(\"Acquiring data from server\")\n",
    "        query = \"\"\"\n",
    "                SELECT * FROM properties_2017\n",
    "                LEFT JOIN (SELECT logerror, transactiondate, parcelid AS parcelid_pred FROM predictions_2017) as preds_2017\n",
    "                ON preds_2017.parcelid_pred = properties_2017.parcelid\n",
    "                LEFT JOIN propertylandusetype as prop_type\n",
    "                USING (propertylandusetypeid)\n",
    "                LEFT JOIN airconditioningtype as ac_type\n",
    "                USING (airconditioningtypeid)\n",
    "                LEFT JOIN architecturalstyletype as arch_type\n",
    "                USING (architecturalstyletypeid)\n",
    "                LEFT JOIN buildingclasstype as b_class_type\n",
    "                USING (buildingclasstypeid)\n",
    "                LEFT JOIN heatingorsystemtype as heat_type\n",
    "                USING (heatingorsystemtypeid)\n",
    "                LEFT JOIN storytype\n",
    "                USING (storytypeid)\n",
    "                LEFT JOIN typeconstructiontype\n",
    "                USING (typeconstructiontypeid)\n",
    "                INNER JOIN\n",
    "                (SELECT parcelid AS parcel_id_max_date, MAX(transactiondate) as max_date\n",
    "                FROM predictions_2017\n",
    "                GROUP BY parcel_id_max_date) AS latest_transaction\n",
    "                ON latest_transaction.parcel_id_max_date = preds_2017.parcelid_pred AND latest_transaction.max_date = preds_2017.transactiondate\n",
    "                WHERE transactiondate IS NOT NULL\n",
    "                AND longitude IS NOT NULL\n",
    "                AND latitude IS NOT NULL\n",
    "                AND transactiondate BETWEEN '2017-01-01' and '2017-12-31';\n",
    "            \"\"\"\n",
    "        df = pd.read_sql(query, get_db_url('zillow'))\n",
    "        \n",
    "        # Drop unnecessary foreign keys\n",
    "        df = df.drop(columns = ['parcel_id_max_date','max_date','parcelid_pred', 'typeconstructiontypeid','storytypeid','heatingorsystemtypeid','buildingclasstypeid','architecturalstyletypeid','architecturalstyletypeid','airconditioningtypeid','propertylandusetypeid'])\n",
    "        df.to_csv('zillow_2017.csv', index=False)\n",
    "    \n",
    "    # Prepare the data for exploration and modeling\n",
    "    # Rename columns as needed\n",
    "    df=df.rename(columns = {'bedroomcnt':'bedroom', \n",
    "                            'bathroomcnt':'bathroom', \n",
    "                            'calculatedfinishedsquarefeet':'square_feet',\n",
    "                            'taxvaluedollarcnt':'tax_value',\n",
    "                            'garagecarcnt':'garage',\n",
    "                           'buildingqualitytypeid':'condition',\n",
    "                           'regionidzip':'zip',\n",
    "                           'poolcnt':'pools',\n",
    "                           'lotsizesquarefeet':'lot_size'})\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03d06393",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def handle_missing_zillow_values(df):\n",
    "    \"\"\" Specific to Zillow dataset. Filters to single unit properties and deals with null values.\"\"\"\n",
    "    \n",
    "    # Just want single unit properties\n",
    "    \n",
    "    # Filter out anything other than unit count = 1 and nans\n",
    "    df = df[(df.unitcnt==1)|(df.unitcnt.isna())]\n",
    "    \n",
    "    # Keep properties that should be single units\n",
    "    properties_to_keep = ['Single Family Residential','Condominum','Mobile Home','Manufactured, Modular, Prefabricated Homes','Residential General','Townhouse',np.nan]\n",
    "    df = df[df.apply(lambda row: row.propertylandusedesc in properties_to_keep, axis=1)]\n",
    "    \n",
    "    # First pass on removing missing values\n",
    "    df_nulls_removed = handle_missing_values(df, prop_required_column=0.3, prop_required_row=0.00002)\n",
    "    \n",
    "    # Fill na values and drop specific columns based on exploring nans\n",
    "    df_nulls_removed['garage'] = df_nulls_removed['garage'].fillna(0)\n",
    "    df_nulls_removed['garagetotalsqft'] = df_nulls_removed['garagetotalsqft'].fillna(0) # 'No garage'\n",
    "    df_nulls_removed['poolsizesum'] = df_nulls_removed['poolsizesum'].fillna(0)# 'No pool'\n",
    "    df_nulls_removed['basementsqft'] = df_nulls_removed['basementsqft'].fillna(0) # 'No basement information'\n",
    "    df_nulls_removed['threequarterbathnbr'] = df_nulls_removed['threequarterbathnbr'].fillna(0)\n",
    "    df_nulls_removed['taxdelinquencyyear'] = df_nulls_removed['taxdelinquencyyear'].fillna(0) # \"Assumed Not Delinquent\"\n",
    "    df_nulls_removed['condition'] = df_nulls_removed['condition'].fillna(-1) # \"Not available\"\n",
    "    df_nulls_removed['yardbuildingsqft17'] = df_nulls_removed['yardbuildingsqft17'].fillna(0) # \"No Patio Information\"\n",
    "    df_nulls_removed['yardbuildingsqft26'] = df_nulls_removed['yardbuildingsqft26'].fillna(0) # \"No Yard Building\"\n",
    "    df_nulls_removed = df_nulls_removed.drop(columns = ['regionidneighborhood','calculatedbathnbr','finishedsquarefeet13','finishedsquarefeet50','finishedsquarefeet6','finishedsquarefeet12','finishedfloor1squarefeet'])\n",
    "    # Make a column for the county based on FIPS\n",
    "    df_nulls_removed[\"county\"] = np.select([df_nulls_removed.fips == 6037, df_nulls_removed.fips==6059, df_nulls_removed.fips == 6111],[\"Los Angeles County\", \"Orange County\", \"Ventura County\"])\n",
    "    \n",
    "    # Fill in binary values with 0s\n",
    "    for col in df_nulls_removed.columns:\n",
    "        if df_nulls_removed[col].nunique() == 1:\n",
    "            df_nulls_removed[col] = df_nulls_removed[col].fillna('None')\n",
    "    \n",
    "    # Fill in count, number, and desc values with 0s and not specified\n",
    "    for col in df_nulls_removed.columns:\n",
    "        if 'desc' in col:\n",
    "            df_nulls_removed[col] = df_nulls_removed[col].fillna('Not Specified')\n",
    "        elif 'cnt' in col:\n",
    "            df_nulls_removed[col] = df_nulls_removed[col].fillna(0)\n",
    "        elif 'number' in col:\n",
    "            df_nulls_removed[col] = df_nulls_removed[col].fillna(0)\n",
    "            \n",
    "    # For now, just remove remaining null values\n",
    "    df_nulls_removed = df_nulls_removed.dropna()\n",
    "            \n",
    "    return df_nulls_removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6972abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, prop_required_column, prop_required_row):\n",
    "    \"\"\" Drops columns and rows from df that have fewer values than required by the \n",
    "    arguments prop_required_column and prop_required_row. First drops columns then drops rows. \n",
    "    Returns a df without the columns and rows that were dropped. \"\"\"\n",
    "    \n",
    "    # Drop columns with pct of missing rows above threshold\n",
    "    print(\"For threshold based dropping: \")\n",
    "    print(df.shape, \" original shape\")\n",
    "    df = df.dropna(thresh = int((prop_required_row)*len(df)), axis=1, inplace=False)\n",
    "    print(df.shape, \" shape after dropping columns with prop required rows below theshold\")\n",
    "    \n",
    "    # Drop rows with pct of missing columns above threshold\n",
    "    df = df.dropna(thresh = int(prop_required_column*len(df.columns)), inplace=False)\n",
    "    print(df.shape, \" shape after dropping rows with prop required columns below threshold\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "846fbeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nulls_by_row(df):\n",
    "    \"\"\" Returns the number of and percent of nulls per row, as well as the number of rows with the given missing num of columns \"\"\"\n",
    "    # nulls by row\n",
    "    info =  pd.concat([\n",
    "        df.isna().sum(axis=1).rename('num_cols_missing'),\n",
    "        df.isna().mean(axis=1).rename('pct_cols_missing'),\n",
    "    ], axis=1)\n",
    "    \n",
    "    return pd.DataFrame(info.value_counts(),columns = ['num_rows']).reset_index().sort_values(by='num_rows', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c03b0adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nulls_by_column(df):\n",
    "    \"\"\" Returns the number of and percent of nulls per column \"\"\"\n",
    "    return pd.concat([\n",
    "        df.isna().sum(axis=0).rename('n_rows_missing'),\n",
    "        df.isna().mean(axis=0).rename('pct_rows_missing'),\n",
    "    ], axis=1).sort_values(by='pct_rows_missing', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b745d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_upper_outliers(s, k):\n",
    "    '''\n",
    "    Given a series and a cutoff value, k, returns the upper outliers for the\n",
    "    series.\n",
    "    \n",
    "    The values returned will be either 0 (if the point is not an outlier), or a\n",
    "    number that indicates how far away from the upper bound the observation is.\n",
    "    '''\n",
    "    q1, q3 = s.quantile([.25, .75])\n",
    "    iqr = q3 - q1\n",
    "    upper_bound = q3 + k * iqr\n",
    "    return s.apply(lambda x: max([x - upper_bound, 0]))\n",
    "\n",
    "def get_lower_outliers(s, k):\n",
    "    '''\n",
    "    Given a series and a cutoff value, k, returns the lower outliers for the\n",
    "    series.\n",
    "    \n",
    "    The values returned will be either 0 (if the point is not an outlier), or a\n",
    "    number that indicates how far away from the lower bound the observation is.\n",
    "    '''\n",
    "    q1, q3 = s.quantile([.25, .75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - k * iqr\n",
    "    return s.apply(lambda x: max([x - lower_bound, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24434c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_upper_outlier_columns(df, k, describe=False):\n",
    "    '''\n",
    "    Add a column with the suffix _upper_outliers for all the numeric columns\n",
    "    in the given dataframe and the given cutoff k value. Optionally displays a description of the outliers.\n",
    "    '''\n",
    "    \n",
    "    for col in df.select_dtypes('number'):\n",
    "        df[col + '_upper_outliers'] = get_upper_outliers(df[col], k)\n",
    "    \n",
    "    outlier_cols = [col for col in df if col.endswith('_upper_outliers')]\n",
    "\n",
    "    if describe:\n",
    "        for col in outlier_cols:\n",
    "            print('---\\n' + col)\n",
    "            data = df[col][df[col] > 0]\n",
    "            print(data.describe())\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_lower_outlier_columns(df, k, describe = False):\n",
    "    '''\n",
    "    Add a column with the suffix _lower_outliers for all the numeric columns\n",
    "    in the given dataframe and the given cutoff k value. Optionally displays a description of the outliers.\n",
    "    '''\n",
    "    \n",
    "    for col in df.select_dtypes('number'):\n",
    "        df[col + '_lower_outliers'] = get_lower_outliers(df[col], k)\n",
    "    \n",
    "    outlier_cols = [col for col in df if col.endswith('_lower_outliers')]\n",
    "\n",
    "    if describe:\n",
    "        for col in outlier_cols:\n",
    "            print('---\\n' + col)\n",
    "            data = df[col][df[col] > 0]\n",
    "            print(data.describe())\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eff3d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, train_size_vs_train_test = 0.8, train_size_vs_train_val = 0.7, random_state = 123):\n",
    "    \"\"\"Splits the inputted dataframe into 3 datasets for train, validate and test (in that order).\n",
    "    Can specific as arguments the percentage of the train/val set vs test (default 0.8) and the percentage of the train size vs train/val (default 0.7). Default values results in following:\n",
    "    Train: 0.56\n",
    "    Validate: 0.24\n",
    "    Test: 0.2\"\"\"\n",
    "    train_val, test = train_test_split(df, train_size=train_size_vs_train_test, random_state=123)\n",
    "    train, validate = train_test_split(train_val, train_size=train_size_vs_train_val, random_state=123)\n",
    "    \n",
    "    train_size = train_size_vs_train_test*train_size_vs_train_val\n",
    "    test_size = 1 - train_size_vs_train_test\n",
    "    validate_size = 1-test_size-train_size\n",
    "    \n",
    "    print(f\"Data split as follows: Train {train_size:.2%}, Validate {validate_size:.2%}, Test {test_size:.2%}\")\n",
    "    \n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c4700a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(train, validate, test, features_to_scale):\n",
    "    \"\"\"Scales data using MinMax Scaler. \n",
    "    Accepts train, validate, and test datasets as inputs as well as a list of the features to scale. \n",
    "    Returns dataframe with scaled values added on as columns\"\"\"\n",
    "    \n",
    "    # Fit the scaler to train data only\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    scaler.fit(train[features_to_scale])\n",
    "    \n",
    "    # Generate a list of the new column names with _scaled added on\n",
    "    scaled_columns = [col+\"_scaled\" for col in features_to_scale]\n",
    "    \n",
    "    # Transform the separate datasets using the scaler learned from train\n",
    "    scaled_train = scaler.transform(train[features_to_scale])\n",
    "    scaled_validate = scaler.transform(validate[features_to_scale])\n",
    "    scaled_test = scaler.transform(test[features_to_scale])\n",
    "    \n",
    "    # Concatenate the scaled data to the original unscaled data\n",
    "    train_scaled = pd.concat([train, pd.DataFrame(scaled_train,index=train.index, columns = scaled_columns)],axis=1)\n",
    "    validate_scaled = pd.concat([validate, pd.DataFrame(scaled_validate,index=validate.index, columns = scaled_columns)],axis=1)\n",
    "    test_scaled = pd.concat([test, pd.DataFrame(scaled_test,index=test.index, columns = scaled_columns)],axis=1)\n",
    "\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9d19a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, k, col_list):\n",
    "    ''' Removes outliers based on multiple of IQR. Accepts as arguments the dataframe, the k value for number of IQR to use as threshold, and the list of columns. Outputs a dataframe without the outliers.\n",
    "    '''\n",
    "    # Create a column that will label our rows as containing an outlier value or not\n",
    "    num_obs = df.shape[0]\n",
    "    df['outlier'] = False\n",
    "    for col in col_list:\n",
    "\n",
    "        q1, q3 = df[col].quantile([.25, .75])  # get quartiles\n",
    "        \n",
    "        iqr = q3 - q1   # calculate interquartile range\n",
    "        \n",
    "        upper_bound = q3 + k * iqr   # get upper bound\n",
    "        lower_bound = q1 - k * iqr   # get lower bound\n",
    "\n",
    "        # update the outlier label any time that the value is outside of boundaries\n",
    "        df['outlier'] = np.where(((df[col] < lower_bound) | (df[col] > upper_bound)) & (df.outlier == False), True, df.outlier)\n",
    "    \n",
    "    df = df[df.outlier == False]\n",
    "    df.drop(columns=['outlier'], inplace=True)\n",
    "    print(f\"Number of observations removed: {num_obs - df.shape[0]}\")\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4c053a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_url(database):\n",
    "    from env import host, user, password\n",
    "    url = f'mysql+pymysql://{user}:{password}@{host}/{database}'\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87027e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquiring data from server\n"
     ]
    }
   ],
   "source": [
    "df = wrangle_zillow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6acc79be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parcelid</th>\n",
       "      <th>basementsqft</th>\n",
       "      <th>bathroom</th>\n",
       "      <th>bedroom</th>\n",
       "      <th>condition</th>\n",
       "      <th>calculatedbathnbr</th>\n",
       "      <th>decktypeid</th>\n",
       "      <th>finishedfloor1squarefeet</th>\n",
       "      <th>square_feet</th>\n",
       "      <th>...</th>\n",
       "      <th>censustractandblock</th>\n",
       "      <th>logerror</th>\n",
       "      <th>transactiondate</th>\n",
       "      <th>propertylandusedesc</th>\n",
       "      <th>airconditioningdesc</th>\n",
       "      <th>architecturalstyledesc</th>\n",
       "      <th>buildingclassdesc</th>\n",
       "      <th>heatingorsystemdesc</th>\n",
       "      <th>storydesc</th>\n",
       "      <th>typeconstructiondesc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1727539</td>\n",
       "      <td>14297519</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.059063e+13</td>\n",
       "      <td>0.025595</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1387261</td>\n",
       "      <td>17052889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1465.0</td>\n",
       "      <td>1465.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.111001e+13</td>\n",
       "      <td>0.055619</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11677</td>\n",
       "      <td>14186244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1243.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.059022e+13</td>\n",
       "      <td>0.005383</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2288172</td>\n",
       "      <td>12177905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2376.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.037300e+13</td>\n",
       "      <td>-0.103410</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Central</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970746</td>\n",
       "      <td>10887214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1312.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.037124e+13</td>\n",
       "      <td>0.006940</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Condominium</td>\n",
       "      <td>Central</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Central</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  parcelid  basementsqft  bathroom  bedroom  condition  \\\n",
       "0  1727539  14297519           NaN       3.5      4.0        NaN   \n",
       "1  1387261  17052889           NaN       1.0      2.0        NaN   \n",
       "2    11677  14186244           NaN       2.0      3.0        NaN   \n",
       "3  2288172  12177905           NaN       3.0      4.0        8.0   \n",
       "4  1970746  10887214           NaN       3.0      3.0        8.0   \n",
       "\n",
       "   calculatedbathnbr  decktypeid  finishedfloor1squarefeet  square_feet  ...  \\\n",
       "0                3.5         NaN                       NaN       3100.0  ...   \n",
       "1                1.0         NaN                    1465.0       1465.0  ...   \n",
       "2                2.0         NaN                       NaN       1243.0  ...   \n",
       "3                3.0         NaN                       NaN       2376.0  ...   \n",
       "4                3.0         NaN                       NaN       1312.0  ...   \n",
       "\n",
       "   censustractandblock  logerror  transactiondate        propertylandusedesc  \\\n",
       "0         6.059063e+13  0.025595       2017-01-01  Single Family Residential   \n",
       "1         6.111001e+13  0.055619       2017-01-01  Single Family Residential   \n",
       "2         6.059022e+13  0.005383       2017-01-01  Single Family Residential   \n",
       "3         6.037300e+13 -0.103410       2017-01-01  Single Family Residential   \n",
       "4         6.037124e+13  0.006940       2017-01-01                Condominium   \n",
       "\n",
       "   airconditioningdesc  architecturalstyledesc  buildingclassdesc  \\\n",
       "0                 None                    None               None   \n",
       "1                 None                    None               None   \n",
       "2                 None                    None               None   \n",
       "3                 None                    None               None   \n",
       "4              Central                    None               None   \n",
       "\n",
       "   heatingorsystemdesc  storydesc  typeconstructiondesc  \n",
       "0                 None       None                  None  \n",
       "1                 None       None                  None  \n",
       "2                 None       None                  None  \n",
       "3              Central       None                  None  \n",
       "4              Central       None                  None  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12d81df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.latitude = df.latitude / 1_000_000\n",
    "df.longitude = df.longitude / 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa2c63bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For threshold based dropping: \n",
      "(52463, 61)  original shape\n",
      "(52463, 59)  shape after dropping columns with prop required rows below theshold\n",
      "(52463, 59)  shape after dropping rows with prop required columns below threshold\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = handle_missing_zillow_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e10bc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50906, 53)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50832c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'parcelid', 'basementsqft', 'bathroom', 'bedroom', 'condition',\n",
       "       'decktypeid', 'square_feet', 'fips', 'fireplacecnt', 'fullbathcnt',\n",
       "       'garage', 'garagetotalsqft', 'hashottuborspa', 'latitude', 'longitude',\n",
       "       'lot_size', 'pools', 'poolsizesum', 'pooltypeid10', 'pooltypeid2',\n",
       "       'pooltypeid7', 'propertycountylandusecode', 'propertyzoningdesc',\n",
       "       'rawcensustractandblock', 'regionidcity', 'regionidcounty', 'zip',\n",
       "       'roomcnt', 'threequarterbathnbr', 'unitcnt', 'yardbuildingsqft17',\n",
       "       'yardbuildingsqft26', 'yearbuilt', 'numberofstories', 'fireplaceflag',\n",
       "       'structuretaxvaluedollarcnt', 'tax_value', 'assessmentyear',\n",
       "       'landtaxvaluedollarcnt', 'taxamount', 'taxdelinquencyflag',\n",
       "       'taxdelinquencyyear', 'censustractandblock', 'logerror',\n",
       "       'transactiondate', 'propertylandusedesc', 'airconditioningdesc',\n",
       "       'architecturalstyledesc', 'heatingorsystemdesc', 'storydesc',\n",
       "       'typeconstructiondesc', 'county'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "affc3742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split as follows: Train 56.00%, Validate 24.00%, Test 20.00%\n"
     ]
    }
   ],
   "source": [
    "train, validate, test = split_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b32af0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28506, 53), (12218, 53), (10182, 53))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b46f0f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0     20463\n",
       "6.0      2239\n",
       "7.0      1996\n",
       "8.0      1553\n",
       "5.0      1020\n",
       "9.0       638\n",
       "4.0       285\n",
       "10.0      188\n",
       "11.0       54\n",
       "3.0        39\n",
       "12.0       15\n",
       "2.0         6\n",
       "13.0        5\n",
       "14.0        4\n",
       "1.0         1\n",
       "Name: roomcnt, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.roomcnt.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05baa23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a185576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e63b62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9e23ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5748473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549e3df4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2627667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2024496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dead281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f29cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
